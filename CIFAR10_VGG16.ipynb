{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10_VGG16.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM4w3Zm2x0nEyrzxzObOaBc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolDolSee/TF2.0/blob/main/CIFAR10_VGG16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1kcSRdY1s2o"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlmRInad13uL"
      },
      "source": [
        "from tensorflow.keras.applications import VGG16, ResNet50, ResNet50V2, Xception"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onSUiyDh2MDV"
      },
      "source": [
        "model = VGG16()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgPqm1UZ2PLK",
        "outputId": "37d9a279-22b5-4ea2-f52d-cf84f4515e9a"
      },
      "source": [
        "model = VGG16(input_shape=(32,32,3), include_top=False, weights='imagenet')\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 2s 0us/step\n",
            "58900480/58889256 [==============================] - 2s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 32, 32, 64)        36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 16, 16, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 8, 8, 256)         590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 8, 8, 256)         590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 4, 4, 512)         1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyFPQcOG4FxV",
        "outputId": "d2e9c2af-a455-42b0-cad1-29529154706c"
      },
      "source": [
        "print(model)\n",
        "print(model.output)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<keras.engine.functional.Functional object at 0x7fba11663a90>\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 512), dtype=tf.float32, name=None), name='block5_pool/MaxPool:0', description=\"created by layer 'block5_pool'\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEXe-Yxp4UqG"
      },
      "source": [
        "IMAGE_SIZE=32\n",
        "BATCH_SIZE=64"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksW5sM1T9Kuy",
        "outputId": "965af5b1-4a05-4eed-f758-35b4a1d2edc3"
      },
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random as python_random\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "(train_images, train_labels),(test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# 0 ~ 1사이값의 float32로 변경하는 함수\n",
        "def get_preprocessed_data(images, labels, scaling=True):\n",
        "    \n",
        "    # 학습과 테스트 이미지 array를 0~1 사이값으로 scale 및 float32 형 변형. \n",
        "    if scaling:\n",
        "        images = np.array(images/255.0, dtype=np.float32)\n",
        "    else:\n",
        "        images = np.array(images, dtype=np.float32)\n",
        "        \n",
        "    labels = np.array(labels, dtype=np.float32)\n",
        "    \n",
        "    return images, labels\n",
        "\n",
        "def get_preprocessed_ohe(images, labels):\n",
        "    images, labels = get_preprocessed_data(images, labels, scaling=False)\n",
        "    # OHE 적용 \n",
        "    oh_labels = to_categorical(labels)\n",
        "    return images, oh_labels\n",
        "\n",
        "# 학습/검증/테스트 데이터 세트에 전처리 및 OHE 적용한 뒤 반환 \n",
        "def get_train_valid_test_set(train_images, train_labels, test_images, test_labels, valid_size=0.15, random_state=2021):\n",
        "    # 학습 및 테스트 데이터 세트를  0 ~ 1사이값 float32로 변경 및 OHE 적용. \n",
        "    train_images, train_oh_labels = get_preprocessed_ohe(train_images, train_labels)\n",
        "    test_images, test_oh_labels = get_preprocessed_ohe(test_images, test_labels)\n",
        "    \n",
        "    # 학습 데이터를 검증 데이터 세트로 다시 분리\n",
        "    tr_images, val_images, tr_oh_labels, val_oh_labels = train_test_split(train_images, train_oh_labels, test_size=valid_size, random_state=random_state)\n",
        "    \n",
        "    return (tr_images, tr_oh_labels), (val_images, val_oh_labels), (test_images, test_oh_labels ) \n",
        "\n",
        "# CIFAR10 데이터 재 로딩 및 Scaling/OHE 전처리 적용하여 학습/검증/데이터 세트 생성. \n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "print(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)\n",
        "\n",
        "(tr_images, tr_oh_labels), (val_images, val_oh_labels), (test_images, test_oh_labels) = \\\n",
        "    get_train_valid_test_set(train_images, train_labels, test_images, test_labels, valid_size=0.15, random_state=2021)\n",
        "\n",
        "print(tr_images.shape, tr_oh_labels.shape, val_images.shape, val_oh_labels.shape, test_images.shape, test_oh_labels.shape)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n",
            "(42500, 32, 32, 3) (42500, 10) (7500, 32, 32, 3) (7500, 10) (10000, 32, 32, 3) (10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqmtldcy-W6E",
        "outputId": "3f527a3a-9b02-48c7-9fd8-f595ef6ed956"
      },
      "source": [
        "print(tr_images.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(51000, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaqFrT2u4wMp",
        "outputId": "6eaadf0b-3eee-453e-e90e-953e475dff8b"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.metrics import Accuracy\n",
        "\n",
        "class VGGclass(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(VGGclass, self).__init__()\n",
        "\n",
        "    self.VGG16 = tf.keras.applications.VGG16(input_shape=(32,32,3), include_top=False, weights=\"imagenet\")\n",
        "    # VGG16_output = self.VGG16.output\n",
        "    self.pooling = tf.keras.layers.GlobalAveragePooling2D()\n",
        "    self.Dense = tf.keras.layers.Dense(50, activation='relu')\n",
        "    self.outputlayer = tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "\n",
        "  def call(self,x):\n",
        "    input = tf.reshape(x,[-1,32,32,3])\n",
        "    x = self.VGG16(input)\n",
        "    x = self.pooling(x)\n",
        "    x = self.Dense(x)\n",
        "    x = self.outputlayer(x)\n",
        "    return x\n",
        "\n",
        "model = VGGclass()\n",
        "model.compile(optimizer = Adam(0.001),loss='categorical_crossentropy',metrics=['Accuracy'])\n",
        "history = model.fit(flow_tr_gen, epochs=40, validation_data = flow_val_gen)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "665/665 [==============================] - 58s 83ms/step - loss: 1.8008 - Accuracy: 0.2825 - val_loss: 1.3731 - val_Accuracy: 0.4632\n",
            "Epoch 2/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 1.2336 - Accuracy: 0.5327 - val_loss: 1.0506 - val_Accuracy: 0.6333\n",
            "Epoch 3/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.9734 - Accuracy: 0.6613 - val_loss: 0.8749 - val_Accuracy: 0.6995\n",
            "Epoch 4/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.8004 - Accuracy: 0.7330 - val_loss: 0.7635 - val_Accuracy: 0.7449\n",
            "Epoch 5/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.7015 - Accuracy: 0.7680 - val_loss: 0.6746 - val_Accuracy: 0.7775\n",
            "Epoch 6/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.6249 - Accuracy: 0.7938 - val_loss: 0.6582 - val_Accuracy: 0.7897\n",
            "Epoch 7/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.5578 - Accuracy: 0.8179 - val_loss: 0.6439 - val_Accuracy: 0.8057\n",
            "Epoch 8/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.5224 - Accuracy: 0.8281 - val_loss: 0.6706 - val_Accuracy: 0.7935\n",
            "Epoch 9/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.4754 - Accuracy: 0.8445 - val_loss: 0.6361 - val_Accuracy: 0.7992\n",
            "Epoch 10/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.4397 - Accuracy: 0.8565 - val_loss: 0.5638 - val_Accuracy: 0.8213\n",
            "Epoch 11/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.4239 - Accuracy: 0.8630 - val_loss: 0.6517 - val_Accuracy: 0.7995\n",
            "Epoch 12/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.3863 - Accuracy: 0.8748 - val_loss: 0.5609 - val_Accuracy: 0.8203\n",
            "Epoch 13/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.3685 - Accuracy: 0.8811 - val_loss: 0.5752 - val_Accuracy: 0.8261\n",
            "Epoch 14/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.3412 - Accuracy: 0.8909 - val_loss: 0.6438 - val_Accuracy: 0.8188\n",
            "Epoch 15/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.3403 - Accuracy: 0.8933 - val_loss: 0.5859 - val_Accuracy: 0.8303\n",
            "Epoch 16/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.2996 - Accuracy: 0.9039 - val_loss: 0.5569 - val_Accuracy: 0.8324\n",
            "Epoch 17/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.2892 - Accuracy: 0.9080 - val_loss: 0.5951 - val_Accuracy: 0.8236\n",
            "Epoch 18/40\n",
            "665/665 [==============================] - 54s 82ms/step - loss: 0.2732 - Accuracy: 0.9138 - val_loss: 0.5763 - val_Accuracy: 0.8321\n",
            "Epoch 19/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.2622 - Accuracy: 0.9145 - val_loss: 0.6309 - val_Accuracy: 0.8315\n",
            "Epoch 20/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.2524 - Accuracy: 0.9210 - val_loss: 0.6068 - val_Accuracy: 0.8333\n",
            "Epoch 21/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.2498 - Accuracy: 0.9216 - val_loss: 0.6368 - val_Accuracy: 0.8384\n",
            "Epoch 22/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.2319 - Accuracy: 0.9270 - val_loss: 0.6158 - val_Accuracy: 0.8357\n",
            "Epoch 23/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.2363 - Accuracy: 0.9259 - val_loss: 0.6618 - val_Accuracy: 0.8273\n",
            "Epoch 24/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.2034 - Accuracy: 0.9355 - val_loss: 0.6034 - val_Accuracy: 0.8479\n",
            "Epoch 25/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.2031 - Accuracy: 0.9365 - val_loss: 0.8133 - val_Accuracy: 0.7581\n",
            "Epoch 26/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.2059 - Accuracy: 0.9360 - val_loss: 0.5913 - val_Accuracy: 0.8415\n",
            "Epoch 27/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.1896 - Accuracy: 0.9418 - val_loss: 0.7657 - val_Accuracy: 0.8149\n",
            "Epoch 28/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.2015 - Accuracy: 0.9381 - val_loss: 0.5723 - val_Accuracy: 0.8475\n",
            "Epoch 29/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.1844 - Accuracy: 0.9445 - val_loss: 0.6061 - val_Accuracy: 0.8601\n",
            "Epoch 30/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.1875 - Accuracy: 0.9419 - val_loss: 0.6798 - val_Accuracy: 0.8459\n",
            "Epoch 31/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.1615 - Accuracy: 0.9506 - val_loss: 0.8203 - val_Accuracy: 0.8205\n",
            "Epoch 32/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.1625 - Accuracy: 0.9519 - val_loss: 0.6149 - val_Accuracy: 0.8439\n",
            "Epoch 33/40\n",
            "665/665 [==============================] - 54s 81ms/step - loss: 0.1381 - Accuracy: 0.9578 - val_loss: 0.6881 - val_Accuracy: 0.8467\n",
            "Epoch 34/40\n",
            "665/665 [==============================] - 54s 82ms/step - loss: 0.1727 - Accuracy: 0.9481 - val_loss: 0.9713 - val_Accuracy: 0.7501\n",
            "Epoch 35/40\n",
            "665/665 [==============================] - 55s 82ms/step - loss: 0.1739 - Accuracy: 0.9476 - val_loss: 0.7127 - val_Accuracy: 0.8360\n",
            "Epoch 36/40\n",
            "665/665 [==============================] - 55s 82ms/step - loss: 0.1620 - Accuracy: 0.9508 - val_loss: 0.6379 - val_Accuracy: 0.8456\n",
            "Epoch 37/40\n",
            "665/665 [==============================] - 55s 82ms/step - loss: 0.1319 - Accuracy: 0.9614 - val_loss: 0.7452 - val_Accuracy: 0.8332\n",
            "Epoch 38/40\n",
            "665/665 [==============================] - 55s 82ms/step - loss: 0.1474 - Accuracy: 0.9556 - val_loss: 0.6297 - val_Accuracy: 0.8507\n",
            "Epoch 39/40\n",
            "665/665 [==============================] - 55s 82ms/step - loss: 0.1404 - Accuracy: 0.9582 - val_loss: 0.8388 - val_Accuracy: 0.8200\n",
            "Epoch 40/40\n",
            "665/665 [==============================] - 55s 82ms/step - loss: 0.1390 - Accuracy: 0.9596 - val_loss: 0.7252 - val_Accuracy: 0.8405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOYR8arL5pq6"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_generator = ImageDataGenerator(\n",
        "    horizontal_flip=True,\n",
        "    rescale=1/255.0\n",
        ")\n",
        "valid_generator = ImageDataGenerator(rescale=1/255.0)\n",
        "\n",
        "flow_tr_gen = train_generator.flow(tr_images, tr_oh_labels, batch_size=BATCH_SIZE, shuffle=True)\n",
        "flow_val_gen = valid_generator.flow(val_images, val_oh_labels, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4c-eQdD-FDJ",
        "outputId": "731fbf30-6160-4732-f2a5-9c82a67d683f"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "test_generator = ImageDataGenerator(rescale=1/255.0)\n",
        "flow_test_gen = test_generator.flow(test_images, test_oh_labels, batch_size=BATCH_SIZE, shuffle=False)\n",
        "model.evaluate(flow_test_gen)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157/157 [==============================] - 4s 28ms/step - loss: 0.7662 - Accuracy: 0.8350\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7662031650543213, 0.8349999785423279]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DAHBNm8Kpef",
        "outputId": "00463444-0bc2-42f0-9085-eba2eed62c72"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vg_gclass_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 1, 1, 512)         14714688  \n",
            "                                                                 \n",
            " global_average_pooling2d_5   multiple                 0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_10 (Dense)            multiple                  25650     \n",
            "                                                                 \n",
            " dense_11 (Dense)            multiple                  510       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,740,848\n",
            "Trainable params: 14,740,848\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM8Gq9c6LX2i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}