{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflowPractice.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP2qR+H2EKP6MxNEpL5jbLK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolDolSee/TF2.0/blob/main/tensorflowPractice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHAd5ig2JwHn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32ed1c61-8b3f-4b09-834e-142312281ded"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy\n",
        "\n",
        "W = tf.Variable(tf.random.normal(shape=[1]))\n",
        "b = tf.Variable(tf.random.normal(shape=[1]))\n",
        "\n",
        "#모델정의\n",
        "@tf.function\n",
        "def linear_model(x):\n",
        "  return W*x + b\n",
        "\n",
        "#loss함수 정의\n",
        "@tf.function\n",
        "def mse_loss(y_pred, y):\n",
        "  return tf.reduce_mean(tf.square(y_pred - y))\n",
        "\n",
        "#optimizer정의\n",
        "optimizer = tf.optimizers.SGD(0.01)\n",
        "\n",
        "#최적화를 위한 함수 정의\n",
        "@tf.function\n",
        "def train_step(x,y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_pred = linear_model(x)\n",
        "    loss = mse_loss(y_pred, y)\n",
        "  gradients = tape.gradient(loss, [W,b])\n",
        "  optimizer.apply_gradients(zip(gradients, [W,b])) \n",
        "\n",
        "x_train = [1,2,3,4]\n",
        "y_train = [2,4,6,8]\n",
        "\n",
        "for i in range(1000):\n",
        "  train_step(x_train, y_train)\n",
        "\n",
        "x_test = [3.5,5,5.5,6]\n",
        "\n",
        "print(linear_model(x_test).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 6.993254  9.975182 10.969157 11.963133]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfkjpjNrzzrp"
      },
      "source": [
        "### mnist 분류기 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtxxD7V8rNwi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f430afff-b27a-438b-91ea-044dd3921e77"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "#딥러닝에서는 실수형을 사용하기 때문에 int형에서 변경해줌\n",
        "x_train, x_test = x_train.astype(\"float32\"), x_test.astype(\"float32\")\n",
        "#flatting: 2차원 메트릭스를 1차원 벡터의 784(28*28)으로 변환해줌\n",
        "x_train, x_test = x_train.reshape([-1,784]), x_test.reshape([-1,784])\n",
        "x_train, x_test = x_train/255., x_test /255.\n",
        "\n",
        "y_train, y_test = tf.one_hot(y_train, depth=10), tf.one_hot(y_test, depth=10)\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeQX2RxzzdzV"
      },
      "source": [
        "#데이터 미니배치 단위로 묶기\n",
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "#batch함수로 원하는 미니배치 갯수 지정 : 60000개의 mnist데이터를 100개 단위로 묶겠다., 다 묶으면 shuffle함\n",
        "train_data = train_data.repeat().shuffle(60000).batch(100)\n",
        "#배치를 순회해서 가져옴\n",
        "train_data_iter = iter(train_data)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe79GF1tPHjh",
        "outputId": "c13b2404-7aea-4b3d-bab8-ccc652feeee7"
      },
      "source": [
        "#가설정의\n",
        "class SoftmaxRegression(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(SoftmaxRegression, self).__init__()\n",
        "    self.softmax_layer = tf.keras.layers.Dense(10,\n",
        "                                               activation = None,\n",
        "                                               #W값\n",
        "                                               kernel_initializer = \"zeros\",\n",
        "                                               #B값\n",
        "                                               bias_initializer=\"zeros\")\n",
        "  def call(self, x):\n",
        "    #인자를 넣어서 softmax_layer연산 수행\n",
        "    logits = self.softmax_layer(x)\n",
        "    #Dense를 지난 결과에 softmax 씌워서 반환\n",
        "    return tf.nn.softmax(logits)\n",
        "\n",
        "#손실함수 정의\n",
        "@tf.function\n",
        "def cross_entropy_loss(y_pred, y):\n",
        "  return tf.reduce_mean(-tf.reduce_sum(y*tf.math.log(y_pred),axis=[1]))\n",
        "\n",
        "#Optimizer\n",
        "optimizer = tf.optimizers.SGD(0.5)\n",
        "\n",
        "#GD정의\n",
        "@tf.function\n",
        "def train_step(model, x, y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_pred = model(x)\n",
        "    loss = cross_entropy_loss(y_pred, y)\n",
        "  #GD가 계산되는 시점, model.trainable_variable은 자동으로 갱신이 요구되는 변수값을 할당해줌.\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  #Optimizer를 통해서 갱신이 이루어지는 시점\n",
        "  optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
        "\n",
        "#정확도 출력 결과 정의\n",
        "@tf.function\n",
        "def compute_accuracy(y_pred, y):\n",
        "  #연산 결과는 10개의 요소를 가진 1차원 벡터이므로 결과 비교를 위해서 argmax를 통해 10개중 가장 큰 값을 뽑아서 equal로 비교함.\n",
        "  corret_prediction = tf.equal(tf.argmax(y_pred,1), tf.argmax(y,1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(corret_prediction, tf.float32))\n",
        "  \n",
        "  return accuracy\n",
        "\n",
        "\n",
        "SoftmaxRegression_model = SoftmaxRegression()\n",
        "\n",
        "for i in range(1000):\n",
        "  batch_xs, batch_ys = next(train_data_iter)\n",
        "  train_step(SoftmaxRegression_model, batch_xs, batch_ys)\n",
        "\n",
        "print(compute_accuracy(SoftmaxRegression_model(x_test), y_test))\n",
        "\n",
        "  \n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0.9175, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oJf-woyfh5m"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "#딥러닝에서는 실수형을 사용하기 때문에 int형에서 변경해줌\n",
        "x_train, x_test = x_train.astype(\"float32\"), x_test.astype(\"float32\")\n",
        "#flatting: 2차원 메트릭스를 1차원 벡터의 784(28*28)으로 변환해줌\n",
        "x_train, x_test = x_train.reshape([-1,784]), x_test.reshape([-1,784])\n",
        "x_train, x_test = x_train/255., x_test /255.\n",
        "\n",
        "y_train, y_test = tf.one_hot(y_train, depth=10), tf.one_hot(y_test, depth=10)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUIDatc1l5Da"
      },
      "source": [
        "learning_rate = 0.001\n",
        "num_epochs = 30\n",
        "batch_size = 256\n",
        "display_step = 1 #손실함수추출력\n",
        "input_size = 784\n",
        "hidden1_size = 256\n",
        "hidden2_size = 256\n",
        "output_size = 10"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZkLfcrcuK0T"
      },
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_data = train_data.shuffle(60000).batch(batch_size)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6JA2GAwnEWx"
      },
      "source": [
        "#초기 가중치 값을 초기화하는 함수\n",
        "def random_normal_initializer_with_stddev_1():\n",
        "  return tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None)\n",
        "\n",
        "class ANN(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(ANN, self).__init__()\n",
        "    self.hidden_layer_1 = tf.keras.layers.Dense(hidden1_size,\n",
        "                                                activation=\"relu\",\n",
        "                                                kernel_initializer=random_normal_initializer_with_stddev_1(),\n",
        "                                                bias_initializer=random_normal_initializer_with_stddev_1())\n",
        "    self.hidden_layer_2 = tf.keras.layers.Dense(hidden2_size,\n",
        "                                         activation=\"relu\",\n",
        "                                         kernel_initializer=random_normal_initializer_with_stddev_1(),\n",
        "                                         bias_initializer=random_normal_initializer_with_stddev_1())\n",
        "    self.output_layer = tf.keras.layers.Dense(output_size,\n",
        "                                       activation=None,\n",
        "                                       kernel_initializer=random_normal_initializer_with_stddev_1(),\n",
        "                                       bias_initializer=random_normal_initializer_with_stddev_1())\n",
        "    \n",
        "  def call(self, x):\n",
        "    H1_output = self.hidden_layer_1(x)\n",
        "    H2_output = self.hidden_layer_2(H1_output)\n",
        "    logits = self.output_layer(H2_output)\n",
        "    return logits\n",
        "\n",
        "#손실함수 정의\n",
        "@tf.function\n",
        "def cross_entropy_loss(logits, y):\n",
        "  return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
        "\n",
        "#optimizer\n",
        "optimizer = tf.optimizers.Adam(learning_rate)\n",
        "\n",
        "#최적화 function\n",
        "@tf.function\n",
        "def train_step(model, x,y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_pred = model(x)\n",
        "    loss = cross_entropy_loss(y_pred, y)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "#정확도 출력 함수\n",
        "@tf.function\n",
        "def compute_accuracy(y_pred, y):\n",
        "  correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y,1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "  return accuracy\n",
        "\n",
        "ANN_model  = ANN()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  average_loss =0.\n",
        "  total_batch = int(x_train.shape[0]/batch_size)\n",
        "\n",
        "  for batch_x, batch_y in train_data:\n",
        "    _, current_loss = train_step(ANN_model, batch_x, batch_y), cross_entropy_loss(ANN_model(batch_x), batch_y)\n",
        "    average_loss +=current_loss/total_batch\n",
        "  if epoch % display_step ==0:\n",
        "    print(\"반복(Epoch): %d, 손실함수(Loss):%f\" % ((epoch+1), average_loss))\n",
        "\n",
        "print(compute_accuracy(ANN_model(x_test),y_test))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVGhzgAXmXeW"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}